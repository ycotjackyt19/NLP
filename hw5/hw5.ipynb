{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9687af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b5305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_path, test_path):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    test_input_texts = []\n",
    "    test_target_texts = []\n",
    "    \n",
    "    num_lines_train_file = sum(1 for line in open(train_path, encoding='utf-8'))\n",
    "    num_lines_test_file = sum(1 for line in open(test_path, encoding='utf-8'))\n",
    "                    \n",
    "    print(\"Read\",train_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(train_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            input_texts.append(data[\"english\"])\n",
    "            target_texts.append(data[\"chinese\"])\n",
    "            \n",
    "            if(counter%1000000==0):\n",
    "                print(\"Now processing {}/{} rows...\".format(counter, num_lines_train_file))   \n",
    "            \n",
    "    print(\"Read\",train_path,\"finished!\")\n",
    "    \n",
    "    print(\"\\nRead\",test_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(test_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            if counter <= num_lines_test_file-100:\n",
    "                input_texts.append(data[\"english\"])\n",
    "                target_texts.append(data[\"chinese\"])\n",
    "            else:\n",
    "                test_input_texts.append(data[\"english\"])\n",
    "                test_target_texts.append(data[\"chinese\"])\n",
    "    print(\"Read\",test_path,\"finished!\")      \n",
    "          \n",
    "    return input_texts, target_texts, test_input_texts, test_target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccba1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncoderDecoderArray():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "033a1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read translation2019zh/translation2019zh_train.json ...\n",
      "Now processing 1000000/5161434 rows...\n",
      "Now processing 2000000/5161434 rows...\n",
      "Now processing 3000000/5161434 rows...\n",
      "Now processing 4000000/5161434 rows...\n",
      "Now processing 5000000/5161434 rows...\n",
      "Read translation2019zh/translation2019zh_train.json finished!\n",
      "\n",
      "Read translation2019zh/translation2019zh_valid.json ...\n",
      "Read translation2019zh/translation2019zh_valid.json finished!\n"
     ]
    }
   ],
   "source": [
    "train_path = 'translation2019zh/translation2019zh_train.json'\n",
    "test_path = 'translation2019zh/translation2019zh_valid.json'\n",
    "\n",
    "input_texts, target_texts, test_input_texts, test_target_texts = preprocess(train_path,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2e4eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(set(input_texts)))\n",
    "target_characters = sorted(list(set(target_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cddc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5200657\n",
      "Number of unique input tokens: 5186506\n",
      "Number of unique output tokens: 5114658\n",
      "Max sequence length for inputs: 422\n",
      "Max sequence length for outputs: 200\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed612361",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 40.4 PiB for an array with shape (5200657, 422, 5186506) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-321b472403f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     [(char, i) for i, char in enumerate(target_characters)])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m encoder_input_data = np.zeros(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_encoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_encoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     dtype='float32')\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 40.4 PiB for an array with shape (5200657, 422, 5186506) and data type float32"
     ]
    }
   ],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2076d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
