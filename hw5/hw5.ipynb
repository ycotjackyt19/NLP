{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43bcedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52f93",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cda8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_path, test_path, num):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    test_input_texts = []\n",
    "    test_target_texts = []\n",
    "    \n",
    "    num_lines_train_file = sum(1 for line in open(train_path, encoding='utf-8'))\n",
    "    num_lines_test_file = sum(1 for line in open(test_path, encoding='utf-8'))\n",
    "                    \n",
    "    print(\"Read\",train_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(train_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            input_texts.append(data[\"english\"])\n",
    "            target_texts.append(data[\"chinese\"])\n",
    "            \n",
    "            '''\n",
    "            if counter%1000000==0:\n",
    "                print(\"Now processing {}/{} rows...\".format(counter, num_lines_train_file))\n",
    "            '''\n",
    "            if counter==num:\n",
    "                break\n",
    "            \n",
    "    print(\"Read\",train_path,\"finished!\")\n",
    "    \n",
    "    print(\"\\nRead\",test_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(test_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            if counter <= num_lines_test_file-100:\n",
    "                #input_texts.append(data[\"english\"])\n",
    "                #target_texts.append(data[\"chinese\"])\n",
    "                pass\n",
    "            else:\n",
    "                test_input_texts.append(data[\"english\"])\n",
    "                test_target_texts.append(data[\"chinese\"])\n",
    "    print(\"Read\",test_path,\"finished!\")      \n",
    "          \n",
    "    return input_texts, target_texts, test_input_texts, test_target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89d7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputTargetChars(input_texts, target_texts):\n",
    "    print(\"\\nProcessing chars...\")\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "    for input_text, target_text in zip(input_texts, target_texts):\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)\n",
    "    print(\"Processing chars finished!\")\n",
    "    return input_characters, target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19295f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncoderDecoderData(input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, input_token_index, target_token_index):\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            #print(char)\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data 领先 decoder_input_data by 一个时间步。\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data 将提前一个时间步，并且将不包含开始字符。\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "        decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "        decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "        \n",
    "    return  encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97659aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModel(latent_dim, num_encoder_tokens, num_decoder_tokens):\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    \n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model,encoder_model,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs):\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)\n",
    "    \n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565acf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read translation2019zh/translation2019zh_train.json ...\n",
      "Read translation2019zh/translation2019zh_train.json finished!\n",
      "\n",
      "Read translation2019zh/translation2019zh_valid.json ...\n",
      "Read translation2019zh/translation2019zh_valid.json finished!\n",
      "\n",
      "Processing chars...\n",
      "Processing chars finished!\n",
      "\n",
      "Number of samples: 4000\n",
      "Number of unique input tokens: 259\n",
      "Number of unique output tokens: 3374\n",
      "Max sequence length for inputs: 256\n",
      "Max sequence length for outputs: 142\n"
     ]
    }
   ],
   "source": [
    "train_path = 'translation2019zh/translation2019zh_train.json'\n",
    "test_path = 'translation2019zh/translation2019zh_valid.json'\n",
    "model_path = 'e2c_ep5.h5'\n",
    "num_line_read = 4000\n",
    "\n",
    "batch_size = 64  \n",
    "epochs = 100\n",
    "latent_dim = 256 \n",
    "\n",
    "input_texts, target_texts, test_input_texts, test_target_texts = preprocess(train_path,test_path, num_line_read)\n",
    "input_characters, target_characters = getInputTargetChars(input_texts, target_texts)\n",
    "input_characters = sorted(list(set(input_characters)))\n",
    "target_characters = sorted(list(set(target_characters)))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('\\nNumber of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba31b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = getEncoderDecoderData(input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, input_token_index, target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f370c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    " model,encoder_model,decoder_model = genModel(latent_dim, num_encoder_tokens, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f32cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 144s 3s/step - loss: 2.3244 - accuracy: 0.7451 - val_loss: 1.8114 - val_accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.7806 - accuracy: 0.7616 - val_loss: 1.8044 - val_accuracy: 0.7615\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 142s 3s/step - loss: 1.7403 - accuracy: 0.7621 - val_loss: 1.7523 - val_accuracy: 0.7624\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.6786 - accuracy: 0.7633 - val_loss: 1.6755 - val_accuracy: 0.7623\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 137s 3s/step - loss: 1.6285 - accuracy: 0.7646 - val_loss: 1.6300 - val_accuracy: 0.7635\n"
     ]
    }
   ],
   "source": [
    "trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1804c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 2.3250 - accuracy: 0.7454 - val_loss: 1.8077 - val_accuracy: 0.7610\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 1.7771 - accuracy: 0.7617 - val_loss: 1.7644 - val_accuracy: 0.7615\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 1.7403 - accuracy: 0.7620 - val_loss: 1.7116 - val_accuracy: 0.7623\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.6758 - accuracy: 0.7631 - val_loss: 1.6478 - val_accuracy: 0.7636\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.6474 - accuracy: 0.7642 - val_loss: 1.7519 - val_accuracy: 0.7648\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 1.6216 - accuracy: 0.7662 - val_loss: 1.6289 - val_accuracy: 0.7657\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.5918 - accuracy: 0.7681 - val_loss: 1.6085 - val_accuracy: 0.7690\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.5955 - accuracy: 0.7666 - val_loss: 1.5994 - val_accuracy: 0.7695\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.5482 - accuracy: 0.7717 - val_loss: 1.5769 - val_accuracy: 0.7709\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.5469 - accuracy: 0.7732 - val_loss: 1.5607 - val_accuracy: 0.7728\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.5078 - accuracy: 0.7755 - val_loss: 1.5410 - val_accuracy: 0.7746\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.4864 - accuracy: 0.7771 - val_loss: 1.5198 - val_accuracy: 0.7765\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 1.4600 - accuracy: 0.7792 - val_loss: 1.5075 - val_accuracy: 0.7771\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 1.4532 - accuracy: 0.7801 - val_loss: 1.5051 - val_accuracy: 0.7796\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 1.4100 - accuracy: 0.7843 - val_loss: 1.4664 - val_accuracy: 0.7812\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.3886 - accuracy: 0.7856 - val_loss: 1.4574 - val_accuracy: 0.7819\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.3684 - accuracy: 0.7872 - val_loss: 1.4399 - val_accuracy: 0.7840\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.3462 - accuracy: 0.7894 - val_loss: 1.4297 - val_accuracy: 0.7858\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 1.3248 - accuracy: 0.7913 - val_loss: 1.4201 - val_accuracy: 0.7864\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.3045 - accuracy: 0.7933 - val_loss: 1.4067 - val_accuracy: 0.7883\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 140s 3s/step - loss: 1.2841 - accuracy: 0.7952 - val_loss: 1.4003 - val_accuracy: 0.7896\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 141s 3s/step - loss: 1.2653 - accuracy: 0.7970 - val_loss: 1.3922 - val_accuracy: 0.7905\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 1.2462 - accuracy: 0.7987 - val_loss: 1.3859 - val_accuracy: 0.7906\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.2277 - accuracy: 0.8003 - val_loss: 1.3824 - val_accuracy: 0.7909\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.2100 - accuracy: 0.8017 - val_loss: 1.3753 - val_accuracy: 0.7924\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 1.1928 - accuracy: 0.8031 - val_loss: 1.3721 - val_accuracy: 0.7926\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 1.1753 - accuracy: 0.8047 - val_loss: 1.3680 - val_accuracy: 0.7926\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 140s 3s/step - loss: 1.1583 - accuracy: 0.8062 - val_loss: 1.3675 - val_accuracy: 0.7930\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.1417 - accuracy: 0.8074 - val_loss: 1.3672 - val_accuracy: 0.7930\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 141s 3s/step - loss: 1.1252 - accuracy: 0.8090 - val_loss: 1.3627 - val_accuracy: 0.7937\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 1.1089 - accuracy: 0.8103 - val_loss: 1.3623 - val_accuracy: 0.7938\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 1.0931 - accuracy: 0.8119 - val_loss: 1.3661 - val_accuracy: 0.7936\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 1.0771 - accuracy: 0.8134 - val_loss: 1.3669 - val_accuracy: 0.7935\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 1.0608 - accuracy: 0.8147 - val_loss: 1.3695 - val_accuracy: 0.7933\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.0460 - accuracy: 0.8162 - val_loss: 1.3700 - val_accuracy: 0.7940\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 1.0301 - accuracy: 0.8177 - val_loss: 1.3710 - val_accuracy: 0.7930\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 160s 3s/step - loss: 1.0151 - accuracy: 0.8190 - val_loss: 1.3752 - val_accuracy: 0.7935\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 162s 3s/step - loss: 0.9999 - accuracy: 0.8209 - val_loss: 1.3791 - val_accuracy: 0.7934\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 0.9848 - accuracy: 0.8223 - val_loss: 1.3847 - val_accuracy: 0.7931\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.9709 - accuracy: 0.8238 - val_loss: 1.3877 - val_accuracy: 0.7932\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.9556 - accuracy: 0.8252 - val_loss: 1.3901 - val_accuracy: 0.7929\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.9413 - accuracy: 0.8270 - val_loss: 1.3994 - val_accuracy: 0.7932\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.9267 - accuracy: 0.8287 - val_loss: 1.3986 - val_accuracy: 0.7927\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.9128 - accuracy: 0.8303 - val_loss: 1.4057 - val_accuracy: 0.7923\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 154s 3s/step - loss: 0.8989 - accuracy: 0.8318 - val_loss: 1.4091 - val_accuracy: 0.7924\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 0.8850 - accuracy: 0.8334 - val_loss: 1.4119 - val_accuracy: 0.7923\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.8711 - accuracy: 0.8353 - val_loss: 1.4226 - val_accuracy: 0.7923\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.8578 - accuracy: 0.8369 - val_loss: 1.4270 - val_accuracy: 0.7920\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.8446 - accuracy: 0.8385 - val_loss: 1.4295 - val_accuracy: 0.7917\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.8311 - accuracy: 0.8403 - val_loss: 1.4410 - val_accuracy: 0.7911\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.8186 - accuracy: 0.8421 - val_loss: 1.4393 - val_accuracy: 0.7914\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.8046 - accuracy: 0.8440 - val_loss: 1.4508 - val_accuracy: 0.7911\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 0.7930 - accuracy: 0.8457 - val_loss: 1.4581 - val_accuracy: 0.7909\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.7794 - accuracy: 0.8476 - val_loss: 1.4674 - val_accuracy: 0.7902\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.7680 - accuracy: 0.8495 - val_loss: 1.4662 - val_accuracy: 0.7912\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.7549 - accuracy: 0.8513 - val_loss: 1.4836 - val_accuracy: 0.7896\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.7432 - accuracy: 0.8533 - val_loss: 1.4851 - val_accuracy: 0.7898\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 151s 3s/step - loss: 0.7314 - accuracy: 0.8550 - val_loss: 1.4890 - val_accuracy: 0.7901\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.7194 - accuracy: 0.8568 - val_loss: 1.4939 - val_accuracy: 0.7899\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.7084 - accuracy: 0.8587 - val_loss: 1.5108 - val_accuracy: 0.7895\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.6972 - accuracy: 0.8605 - val_loss: 1.5110 - val_accuracy: 0.7893\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.6860 - accuracy: 0.8623 - val_loss: 1.5207 - val_accuracy: 0.7893\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.6748 - accuracy: 0.8645 - val_loss: 1.5214 - val_accuracy: 0.7890\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.6647 - accuracy: 0.8663 - val_loss: 1.5289 - val_accuracy: 0.7892\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.6525 - accuracy: 0.8682 - val_loss: 1.5468 - val_accuracy: 0.7885\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.6434 - accuracy: 0.8697 - val_loss: 1.5419 - val_accuracy: 0.7891\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.6321 - accuracy: 0.8718 - val_loss: 1.5492 - val_accuracy: 0.7883\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.6221 - accuracy: 0.8740 - val_loss: 1.5500 - val_accuracy: 0.7886\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.6119 - accuracy: 0.8754 - val_loss: 1.5660 - val_accuracy: 0.7881\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.6024 - accuracy: 0.8776 - val_loss: 1.5732 - val_accuracy: 0.7877\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.5918 - accuracy: 0.8793 - val_loss: 1.5796 - val_accuracy: 0.7873\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.5820 - accuracy: 0.8813 - val_loss: 1.5923 - val_accuracy: 0.7869\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.5739 - accuracy: 0.8827 - val_loss: 1.5932 - val_accuracy: 0.7879\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.5634 - accuracy: 0.8848 - val_loss: 1.6028 - val_accuracy: 0.7868\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 140s 3s/step - loss: 0.5547 - accuracy: 0.8865 - val_loss: 1.6134 - val_accuracy: 0.7866\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.5440 - accuracy: 0.8888 - val_loss: 1.6180 - val_accuracy: 0.7871\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 141s 3s/step - loss: 0.5366 - accuracy: 0.8900 - val_loss: 1.6283 - val_accuracy: 0.7868\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.5279 - accuracy: 0.8918 - val_loss: 1.6369 - val_accuracy: 0.7864\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 0.5187 - accuracy: 0.8937 - val_loss: 1.6445 - val_accuracy: 0.7862\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.5097 - accuracy: 0.8957 - val_loss: 1.6542 - val_accuracy: 0.7858\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 0.5018 - accuracy: 0.8971 - val_loss: 1.6606 - val_accuracy: 0.7855\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.4933 - accuracy: 0.8989 - val_loss: 1.6668 - val_accuracy: 0.7861\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.4855 - accuracy: 0.9006 - val_loss: 1.6727 - val_accuracy: 0.7861\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.4780 - accuracy: 0.9020 - val_loss: 1.6808 - val_accuracy: 0.7861\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.4710 - accuracy: 0.9035 - val_loss: 1.6892 - val_accuracy: 0.7851\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 141s 3s/step - loss: 0.4618 - accuracy: 0.9055 - val_loss: 1.7043 - val_accuracy: 0.7851\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.4536 - accuracy: 0.9074 - val_loss: 1.7027 - val_accuracy: 0.7852\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 140s 3s/step - loss: 0.4471 - accuracy: 0.9084 - val_loss: 1.7146 - val_accuracy: 0.7851\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.4391 - accuracy: 0.9104 - val_loss: 1.7204 - val_accuracy: 0.7848\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.4313 - accuracy: 0.9120 - val_loss: 1.7342 - val_accuracy: 0.7846\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.4234 - accuracy: 0.9137 - val_loss: 1.7382 - val_accuracy: 0.7847\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.4177 - accuracy: 0.9149 - val_loss: 1.7412 - val_accuracy: 0.7847\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.4109 - accuracy: 0.9167 - val_loss: 1.7501 - val_accuracy: 0.7846\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 0.4033 - accuracy: 0.9180 - val_loss: 1.7694 - val_accuracy: 0.7839\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.3961 - accuracy: 0.9198 - val_loss: 1.7707 - val_accuracy: 0.7839\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.3896 - accuracy: 0.9215 - val_loss: 1.7818 - val_accuracy: 0.7839\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.3840 - accuracy: 0.9225 - val_loss: 1.7839 - val_accuracy: 0.7841\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.4729 - accuracy: 0.9071 - val_loss: 1.7318 - val_accuracy: 0.7846\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.5238 - accuracy: 0.8915 - val_loss: 1.7207 - val_accuracy: 0.7854\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 0.4735 - accuracy: 0.9000 - val_loss: 1.7324 - val_accuracy: 0.7858\n"
     ]
    }
   ],
   "source": [
    "trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110b5df",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d080b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 将输入编码为状态向量。\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 生成长度为 1 的空目标序列。\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 用起始字符填充目标序列的第一个字符。\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 一批序列的采样循环\n",
    "    # (为了简化，这里我们假设一批大小为 1)。\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 采样一个 token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 退出条件：达到最大长度或找到停止符。\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目标序列（长度为 1）。\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新状态\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35cb89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向查询 token 索引可将序列解码回可读的内容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01909750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。\n",
      "-\n",
      "Input sentence: He calls the Green Book, his book of teachings, “the new gospel.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。\n",
      "-\n",
      "Input sentence: And the light breeze moves me to caress her long ear\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 微风推着我去爱抚它的长耳朵\n",
      "-\n",
      "Input sentence: They have the blood of martyrs is the White to flow …\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 它们的先烈们的鲜血是白流了…\n",
      "-\n",
      "Input sentence: Finally, the Lakers head to the Motor City to take on a Pistons team that currently owns the Eastern Conference's second best record (1/31). L.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。\n",
      "-\n",
      "Input sentence: \"The perfect match—my father loves names and Jackie loves money, \" sneered Alexander at the wedding. Neither he nor Christina ever got along with their stepmother17.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                    \n",
      "Correct sentence: “真是天造地设的一对——我父亲喜欢结交名人，杰姬酷爱金钱，”亚历山大在婚礼上讥讽道。他和克里斯蒂娜从未同他们的继母和睦相处过。\n",
      "-\n",
      "Input sentence: In 2006, Walmart was charged with racism when its recommendation engine paired Planet of the Apes with a documentary about Martin Luther King.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 2006年，沃尔玛的推荐引擎竟将《人猿星球》与马丁·路德·金的记录片配成了一对，为此沃尔玛遭到了种族歧视的指控。\n",
      "-\n",
      "Input sentence: The matte as main copper phase in the cleaning. slag was deter- mined by electron probe microscopic analysis.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 通过电子探针显微分析确定贫化渣中主要铜相为冰铜相。\n",
      "-\n",
      "Input sentence: Have you shined your shoes?\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 吉姆靠给人擦皮鞋为生。\n",
      "-\n",
      "Input sentence: The Tanning Matrix can be formed by resorcinol and oxazolidine E, and the reactioncharateristics between Tanning Matrix and collagen were investigated through NMR and size distribution analysis.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                    \n",
      "Correct sentence: 用甘氨酸模拟胶原，研究间苯二酚-恶唑烷E鞣性基质的形成以及与胶原之间的反应特性。\n",
      "-\n",
      "Input sentence: Free delivery for addresses in the city. Can be delivered through Internet.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 免费市内取送、免费提供打印译稿及电子文档各一份。\n",
      "-\n",
      "Input sentence: Keele University is renowned for its exciting approach to higher education, beautiful campus, strong community spirit and excellent student life.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 基尔大学以其令人兴奋的方式高等教育，美丽的校园，强大的社区精神和优秀学生的生活。\n",
      "-\n",
      "Input sentence: Among them, there was the herb Tuckahoe grown in Yunnan and Guizhou provinces.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 这些中药中就有生长于云南和贵州的茯苓。\n",
      "-\n",
      "Input sentence: Your willingness to sacrifice countless late nights consoling them?\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 或者是因为你愿意花费无数的夜晚去安慰他们？\n",
      "-\n",
      "Input sentence: Callum: OK, we'll find out if you're right at the end of the programme.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: Callum：  OK，答案将会在节目的最后揭晓，到时我们再看你有没有答对。\n",
      "-\n",
      "Input sentence: When standing on a level surface, the hind feet are set back from under the body and the leg from pad to hock is at right angles to the ground.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 当他站在水平的地面上时，后足爪位于身躯后方（不在身躯正下方），从脚垫到飞节垂直于地面。\n",
      "-\n",
      "Input sentence: So who won? (Alaska doesn't count, you BOUGHT that state from Russia.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 所以到底是谁赢了？ 阿拉斯加不算数，那是你们从俄罗斯买的。\n",
      "-\n",
      "Input sentence: A Minneapolis couple decided to go to Florida to thaw out during a particularly icy winter.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 在一个特别寒冷的冬天，一对住在明尼阿波利斯市的夫妇决定去弗罗里达避寒。\n",
      "-\n",
      "Input sentence: Dumbledore, the lover of warm socks and sherbet lemons, creates soft, comfortable furniture.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 热爱温暖的毛袜子和冰冻柠檬汁的邓布利多，变出柔软舒适的家具。\n",
      "-\n",
      "Input sentence: To escape with a Ph.D., you must meaningfully extend the boundary of human knowledge.\n",
      "Decoded sentence: 们的的的的的的的的的的的的的的的的的的的的的的的的。                                                                                                                     \n",
      "Correct sentence: 要成功拿到博士学位，你必须真正的扩展人类的知识边界。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-29d9023b713c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 抽取一个序列（训练集的一部分）进行解码。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-18bb6a336146>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         output_tokens, h, c = decoder_model.predict(\n\u001b[0m\u001b[0;32m     16\u001b[0m             [target_seq] + states_value)\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # 抽取一个序列（训练集的一部分）进行解码。\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Correct sentence:', target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "547178a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a492778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2c26818e2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec903dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a147474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33c159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f559e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282ab50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f387b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa9dc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e2050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
