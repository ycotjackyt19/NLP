{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b0df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfc2ae",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41050c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_path, test_path, num):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    test_input_texts = []\n",
    "    test_target_texts = []\n",
    "    \n",
    "    num_lines_train_file = sum(1 for line in open(train_path, encoding='utf-8'))\n",
    "    num_lines_test_file = sum(1 for line in open(test_path, encoding='utf-8'))\n",
    "                    \n",
    "    print(\"Read\",train_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(train_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            input_texts.append(data[\"english\"])\n",
    "            target_texts.append(data[\"chinese\"])\n",
    "            \n",
    "            '''\n",
    "            if counter%1000000==0:\n",
    "                print(\"Now processing {}/{} rows...\".format(counter, num_lines_train_file))\n",
    "            '''\n",
    "            if counter==num:\n",
    "                break\n",
    "            \n",
    "    print(\"Read\",train_path,\"finished!\")\n",
    "    \n",
    "    print(\"\\nRead\",test_path,\"...\")\n",
    "    counter = 0\n",
    "    with open(test_path,  encoding='utf-8') as fp:\n",
    "        for json_str in fp:\n",
    "            counter = counter + 1\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            if counter <= num_lines_test_file-100:\n",
    "                #input_texts.append(data[\"english\"])\n",
    "                #target_texts.append(data[\"chinese\"])\n",
    "                pass\n",
    "            else:\n",
    "                test_input_texts.append(data[\"english\"])\n",
    "                test_target_texts.append(data[\"chinese\"])\n",
    "    print(\"Read\",test_path,\"finished!\")      \n",
    "          \n",
    "    return input_texts, target_texts, test_input_texts, test_target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d3de829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputTargetChars(input_texts, target_texts):\n",
    "    print(\"\\nProcessing chars...\")\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "    for input_text, target_text in zip(input_texts, target_texts):\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)\n",
    "    print(\"Processing chars finished!\")\n",
    "    return input_characters, target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff2a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncoderDecoderData(input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, input_token_index, target_token_index):\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            #print(char)\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data 领先 decoder_input_data by 一个时间步。\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data 将提前一个时间步，并且将不包含开始字符。\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "        decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "        decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "        \n",
    "    return  encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e8499c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModel(latent_dim, num_encoder_tokens, num_decoder_tokens):\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    \n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model,encoder_model,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd64f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs):\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)\n",
    "    \n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959e0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read translation2019zh/translation2019zh_train.json ...\n",
      "Read translation2019zh/translation2019zh_train.json finished!\n",
      "\n",
      "Read translation2019zh/translation2019zh_valid.json ...\n",
      "Read translation2019zh/translation2019zh_valid.json finished!\n",
      "\n",
      "Processing chars...\n",
      "Processing chars finished!\n",
      "\n",
      "Number of samples: 4000\n",
      "Number of unique input tokens: 259\n",
      "Number of unique output tokens: 3374\n",
      "Max sequence length for inputs: 256\n",
      "Max sequence length for outputs: 142\n"
     ]
    }
   ],
   "source": [
    "train_path = 'translation2019zh/translation2019zh_train.json'\n",
    "test_path = 'translation2019zh/translation2019zh_valid.json'\n",
    "model_path = 'e2c_ep100.h5'\n",
    "num_line_read = 4000\n",
    "\n",
    "batch_size = 64  \n",
    "epochs = 100\n",
    "latent_dim = 256 \n",
    "\n",
    "input_texts, target_texts, test_input_texts, test_target_texts = preprocess(train_path,test_path, num_line_read)\n",
    "input_characters, target_characters = getInputTargetChars(input_texts, target_texts)\n",
    "input_characters = sorted(list(set(input_characters)))\n",
    "target_characters = sorted(list(set(target_characters)))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('\\nNumber of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f8d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = getEncoderDecoderData(input_texts, target_texts, encoder_input_data, decoder_input_data, decoder_target_data, input_token_index, target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72964af1",
   "metadata": {},
   "outputs": [],
   "source": [
    " model,encoder_model,decoder_model = genModel(latent_dim, num_encoder_tokens, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f9b8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 2.3371 - accuracy: 0.7453 - val_loss: 1.8029 - val_accuracy: 0.7609\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.7879 - accuracy: 0.7613 - val_loss: 1.7850 - val_accuracy: 0.7615\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.7583 - accuracy: 0.7620 - val_loss: 1.7372 - val_accuracy: 0.7613\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 1.6979 - accuracy: 0.7626 - val_loss: 1.7101 - val_accuracy: 0.7618\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 1.6388 - accuracy: 0.7639 - val_loss: 1.6442 - val_accuracy: 0.7670\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 1.6171 - accuracy: 0.7663 - val_loss: 1.6209 - val_accuracy: 0.7673\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 1.6030 - accuracy: 0.7677 - val_loss: 1.6140 - val_accuracy: 0.7664\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.5909 - accuracy: 0.7698 - val_loss: 1.6331 - val_accuracy: 0.7635\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.5578 - accuracy: 0.7710 - val_loss: 1.5820 - val_accuracy: 0.7705\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.5375 - accuracy: 0.7726 - val_loss: 1.5711 - val_accuracy: 0.7715\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 1.5161 - accuracy: 0.7751 - val_loss: 1.5453 - val_accuracy: 0.7740\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 157s 3s/step - loss: 1.4887 - accuracy: 0.7769 - val_loss: 1.5302 - val_accuracy: 0.7742\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 1.4715 - accuracy: 0.7791 - val_loss: 1.5051 - val_accuracy: 0.7776\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 1.4407 - accuracy: 0.7813 - val_loss: 1.4907 - val_accuracy: 0.7789\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 1.4179 - accuracy: 0.7833 - val_loss: 1.4770 - val_accuracy: 0.7805\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.3945 - accuracy: 0.7853 - val_loss: 1.4582 - val_accuracy: 0.7823\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.3716 - accuracy: 0.7871 - val_loss: 1.4409 - val_accuracy: 0.7834\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.3488 - accuracy: 0.7893 - val_loss: 1.4307 - val_accuracy: 0.7848\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.3287 - accuracy: 0.7911 - val_loss: 1.4188 - val_accuracy: 0.7863\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 1.3080 - accuracy: 0.7932 - val_loss: 1.4165 - val_accuracy: 0.7873\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 1.2896 - accuracy: 0.7947 - val_loss: 1.4032 - val_accuracy: 0.7887\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.2700 - accuracy: 0.7965 - val_loss: 1.3935 - val_accuracy: 0.7897\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.2512 - accuracy: 0.7983 - val_loss: 1.3887 - val_accuracy: 0.7901\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 1.2337 - accuracy: 0.7998 - val_loss: 1.3841 - val_accuracy: 0.7911\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.2161 - accuracy: 0.8010 - val_loss: 1.3768 - val_accuracy: 0.7917\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.1992 - accuracy: 0.8026 - val_loss: 1.3799 - val_accuracy: 0.7920\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.1823 - accuracy: 0.8040 - val_loss: 1.3731 - val_accuracy: 0.7928\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.1663 - accuracy: 0.8054 - val_loss: 1.3690 - val_accuracy: 0.7930\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 1.1493 - accuracy: 0.8068 - val_loss: 1.3669 - val_accuracy: 0.7930\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.1343 - accuracy: 0.8084 - val_loss: 1.3661 - val_accuracy: 0.7931\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.1181 - accuracy: 0.8098 - val_loss: 1.3659 - val_accuracy: 0.7936\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.1025 - accuracy: 0.8111 - val_loss: 1.3651 - val_accuracy: 0.7936\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 1.0870 - accuracy: 0.8127 - val_loss: 1.3675 - val_accuracy: 0.7937\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.0721 - accuracy: 0.8141 - val_loss: 1.3704 - val_accuracy: 0.7939\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 1.0563 - accuracy: 0.8154 - val_loss: 1.3729 - val_accuracy: 0.7933\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 1.0415 - accuracy: 0.8167 - val_loss: 1.3698 - val_accuracy: 0.7940\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 1.0264 - accuracy: 0.8184 - val_loss: 1.3733 - val_accuracy: 0.7936\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 1.0120 - accuracy: 0.8198 - val_loss: 1.3771 - val_accuracy: 0.7933\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.9974 - accuracy: 0.8213 - val_loss: 1.3762 - val_accuracy: 0.7936\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.9831 - accuracy: 0.8228 - val_loss: 1.3842 - val_accuracy: 0.7935\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.9683 - accuracy: 0.8240 - val_loss: 1.3847 - val_accuracy: 0.7930\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.9546 - accuracy: 0.8259 - val_loss: 1.3891 - val_accuracy: 0.7930\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.9395 - accuracy: 0.8276 - val_loss: 1.3990 - val_accuracy: 0.7924\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.9266 - accuracy: 0.8288 - val_loss: 1.4025 - val_accuracy: 0.7923\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.9124 - accuracy: 0.8304 - val_loss: 1.3987 - val_accuracy: 0.7929\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.8990 - accuracy: 0.8323 - val_loss: 1.4057 - val_accuracy: 0.7924\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 0.8854 - accuracy: 0.8337 - val_loss: 1.4154 - val_accuracy: 0.7919\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.8726 - accuracy: 0.8353 - val_loss: 1.4257 - val_accuracy: 0.7916\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.8589 - accuracy: 0.8371 - val_loss: 1.4255 - val_accuracy: 0.7920\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.8464 - accuracy: 0.8385 - val_loss: 1.4281 - val_accuracy: 0.7917\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.8330 - accuracy: 0.8402 - val_loss: 1.4347 - val_accuracy: 0.7915\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.8203 - accuracy: 0.8422 - val_loss: 1.4485 - val_accuracy: 0.7904\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 0.8083 - accuracy: 0.8437 - val_loss: 1.4511 - val_accuracy: 0.7911\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.7963 - accuracy: 0.8452 - val_loss: 1.4513 - val_accuracy: 0.7908\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.7837 - accuracy: 0.8471 - val_loss: 1.4588 - val_accuracy: 0.7907\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.7726 - accuracy: 0.8490 - val_loss: 1.4664 - val_accuracy: 0.7907\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.7590 - accuracy: 0.8508 - val_loss: 1.4760 - val_accuracy: 0.7900\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 149s 3s/step - loss: 0.7482 - accuracy: 0.8524 - val_loss: 1.4782 - val_accuracy: 0.7899\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.7350 - accuracy: 0.8545 - val_loss: 1.4837 - val_accuracy: 0.7899\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.7247 - accuracy: 0.8562 - val_loss: 1.4944 - val_accuracy: 0.7891\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.7137 - accuracy: 0.8581 - val_loss: 1.5025 - val_accuracy: 0.7886\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.7015 - accuracy: 0.8601 - val_loss: 1.5045 - val_accuracy: 0.7896\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.6910 - accuracy: 0.8619 - val_loss: 1.5107 - val_accuracy: 0.7887\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.6807 - accuracy: 0.8634 - val_loss: 1.5179 - val_accuracy: 0.7888\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.6690 - accuracy: 0.8656 - val_loss: 1.5251 - val_accuracy: 0.7889\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.6601 - accuracy: 0.8672 - val_loss: 1.5344 - val_accuracy: 0.7884\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.6490 - accuracy: 0.8692 - val_loss: 1.5400 - val_accuracy: 0.7884\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 0.6379 - accuracy: 0.8709 - val_loss: 1.5467 - val_accuracy: 0.7877\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.6296 - accuracy: 0.8724 - val_loss: 1.5541 - val_accuracy: 0.7880\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.6187 - accuracy: 0.8745 - val_loss: 1.5599 - val_accuracy: 0.7873\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.6093 - accuracy: 0.8765 - val_loss: 1.5655 - val_accuracy: 0.7875\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.5994 - accuracy: 0.8780 - val_loss: 1.5748 - val_accuracy: 0.7873\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.5908 - accuracy: 0.8799 - val_loss: 1.5847 - val_accuracy: 0.7868\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 164s 3s/step - loss: 0.5809 - accuracy: 0.8819 - val_loss: 1.5975 - val_accuracy: 0.7865\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.5704 - accuracy: 0.8836 - val_loss: 1.5991 - val_accuracy: 0.7866\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 167s 3s/step - loss: 0.5623 - accuracy: 0.8857 - val_loss: 1.6065 - val_accuracy: 0.7863\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 163s 3s/step - loss: 0.5533 - accuracy: 0.8870 - val_loss: 1.6095 - val_accuracy: 0.7861\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 163s 3s/step - loss: 0.5439 - accuracy: 0.8887 - val_loss: 1.6234 - val_accuracy: 0.7858\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 159s 3s/step - loss: 0.5366 - accuracy: 0.8903 - val_loss: 1.6343 - val_accuracy: 0.7854\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.5265 - accuracy: 0.8926 - val_loss: 1.6332 - val_accuracy: 0.7854\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.5186 - accuracy: 0.8941 - val_loss: 1.6438 - val_accuracy: 0.7857\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.5106 - accuracy: 0.8956 - val_loss: 1.6491 - val_accuracy: 0.7858\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.5026 - accuracy: 0.8972 - val_loss: 1.6595 - val_accuracy: 0.7852\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.4940 - accuracy: 0.8989 - val_loss: 1.6732 - val_accuracy: 0.7848\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.4868 - accuracy: 0.9006 - val_loss: 1.6729 - val_accuracy: 0.7855\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.4777 - accuracy: 0.9030 - val_loss: 1.6786 - val_accuracy: 0.7854\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 157s 3s/step - loss: 0.4709 - accuracy: 0.9039 - val_loss: 1.6911 - val_accuracy: 0.7841\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 156s 3s/step - loss: 0.4628 - accuracy: 0.9055 - val_loss: 1.6943 - val_accuracy: 0.7850\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 161s 3s/step - loss: 0.4557 - accuracy: 0.9073 - val_loss: 1.7079 - val_accuracy: 0.7844\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 163s 3s/step - loss: 0.4475 - accuracy: 0.9089 - val_loss: 1.7160 - val_accuracy: 0.7847\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 160s 3s/step - loss: 0.4408 - accuracy: 0.9103 - val_loss: 1.7247 - val_accuracy: 0.7844\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.4332 - accuracy: 0.9118 - val_loss: 1.7334 - val_accuracy: 0.7840\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 158s 3s/step - loss: 0.4263 - accuracy: 0.9135 - val_loss: 1.7419 - val_accuracy: 0.7842\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 162s 3s/step - loss: 0.4193 - accuracy: 0.9149 - val_loss: 1.7454 - val_accuracy: 0.7844\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 162s 3s/step - loss: 0.4132 - accuracy: 0.9161 - val_loss: 1.7555 - val_accuracy: 0.7837\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 164s 3s/step - loss: 0.4059 - accuracy: 0.9182 - val_loss: 1.7575 - val_accuracy: 0.7838\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.3995 - accuracy: 0.9192 - val_loss: 1.7744 - val_accuracy: 0.7838\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.3924 - accuracy: 0.9209 - val_loss: 1.7768 - val_accuracy: 0.7837\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.3861 - accuracy: 0.9225 - val_loss: 1.7895 - val_accuracy: 0.7829\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.3790 - accuracy: 0.9241 - val_loss: 1.7952 - val_accuracy: 0.7831\n"
     ]
    }
   ],
   "source": [
    "trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b15e9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.3935 - accuracy: 0.9190 - val_loss: 1.7967 - val_accuracy: 0.7835\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.3699 - accuracy: 0.9258 - val_loss: 1.8144 - val_accuracy: 0.7832\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.3630 - accuracy: 0.9277 - val_loss: 1.8177 - val_accuracy: 0.7828\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 0.3574 - accuracy: 0.9285 - val_loss: 1.8294 - val_accuracy: 0.7827\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.3501 - accuracy: 0.9305 - val_loss: 1.8387 - val_accuracy: 0.7831\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.3449 - accuracy: 0.9318 - val_loss: 1.8434 - val_accuracy: 0.7826\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.3390 - accuracy: 0.9328 - val_loss: 1.8496 - val_accuracy: 0.7826\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.3332 - accuracy: 0.9340 - val_loss: 1.8591 - val_accuracy: 0.7826\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.3279 - accuracy: 0.9354 - val_loss: 1.8674 - val_accuracy: 0.7824\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.3221 - accuracy: 0.9364 - val_loss: 1.8722 - val_accuracy: 0.7822\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.3187 - accuracy: 0.9374 - val_loss: 1.8863 - val_accuracy: 0.7822\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.3105 - accuracy: 0.9394 - val_loss: 1.8920 - val_accuracy: 0.7817\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.3070 - accuracy: 0.9399 - val_loss: 1.9009 - val_accuracy: 0.7818\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.3015 - accuracy: 0.9409 - val_loss: 1.9030 - val_accuracy: 0.7822\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2962 - accuracy: 0.9426 - val_loss: 1.9162 - val_accuracy: 0.7820\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.2911 - accuracy: 0.9438 - val_loss: 1.9217 - val_accuracy: 0.7819\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.2865 - accuracy: 0.9445 - val_loss: 1.9254 - val_accuracy: 0.7818\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.2822 - accuracy: 0.9454 - val_loss: 1.9440 - val_accuracy: 0.7816\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2764 - accuracy: 0.9471 - val_loss: 1.9518 - val_accuracy: 0.7812\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.2717 - accuracy: 0.9477 - val_loss: 1.9594 - val_accuracy: 0.7813\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.2680 - accuracy: 0.9486 - val_loss: 1.9769 - val_accuracy: 0.7812\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2636 - accuracy: 0.9497 - val_loss: 1.9742 - val_accuracy: 0.7811\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.2580 - accuracy: 0.9510 - val_loss: 1.9739 - val_accuracy: 0.7815\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.2546 - accuracy: 0.9518 - val_loss: 1.9889 - val_accuracy: 0.7810\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.2493 - accuracy: 0.9530 - val_loss: 2.0021 - val_accuracy: 0.7810\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.2463 - accuracy: 0.9535 - val_loss: 2.0045 - val_accuracy: 0.7808\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.2406 - accuracy: 0.9550 - val_loss: 2.0062 - val_accuracy: 0.7814\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.2390 - accuracy: 0.9547 - val_loss: 2.0216 - val_accuracy: 0.7809\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.2331 - accuracy: 0.9565 - val_loss: 2.0276 - val_accuracy: 0.7809\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2308 - accuracy: 0.9568 - val_loss: 2.0322 - val_accuracy: 0.7812\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.2245 - accuracy: 0.9584 - val_loss: 2.0531 - val_accuracy: 0.7800\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.2222 - accuracy: 0.9590 - val_loss: 2.0528 - val_accuracy: 0.7812\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 143s 3s/step - loss: 0.2164 - accuracy: 0.9603 - val_loss: 2.0674 - val_accuracy: 0.7804\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.2147 - accuracy: 0.9604 - val_loss: 2.0745 - val_accuracy: 0.7807\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2105 - accuracy: 0.9614 - val_loss: 2.0770 - val_accuracy: 0.7809\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.2073 - accuracy: 0.9623 - val_loss: 2.0843 - val_accuracy: 0.7802\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.2034 - accuracy: 0.9631 - val_loss: 2.0864 - val_accuracy: 0.7808\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1999 - accuracy: 0.9638 - val_loss: 2.1068 - val_accuracy: 0.7804\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1975 - accuracy: 0.9643 - val_loss: 2.1099 - val_accuracy: 0.7798\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.1930 - accuracy: 0.9653 - val_loss: 2.1271 - val_accuracy: 0.7800\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1916 - accuracy: 0.9653 - val_loss: 2.1241 - val_accuracy: 0.7802\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.1872 - accuracy: 0.9667 - val_loss: 2.1279 - val_accuracy: 0.7802\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.1835 - accuracy: 0.9675 - val_loss: 2.1336 - val_accuracy: 0.7800\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.1811 - accuracy: 0.9680 - val_loss: 2.1421 - val_accuracy: 0.7801\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.1763 - accuracy: 0.9692 - val_loss: 2.1555 - val_accuracy: 0.7799\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1739 - accuracy: 0.9694 - val_loss: 2.1607 - val_accuracy: 0.7799\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1724 - accuracy: 0.9697 - val_loss: 2.1702 - val_accuracy: 0.7796\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1689 - accuracy: 0.9706 - val_loss: 2.1910 - val_accuracy: 0.7795\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1655 - accuracy: 0.9714 - val_loss: 2.2033 - val_accuracy: 0.7795\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1633 - accuracy: 0.9715 - val_loss: 2.1896 - val_accuracy: 0.7796\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.1606 - accuracy: 0.9725 - val_loss: 2.2027 - val_accuracy: 0.7796\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1567 - accuracy: 0.9734 - val_loss: 2.2100 - val_accuracy: 0.7796\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.1552 - accuracy: 0.9734 - val_loss: 2.2144 - val_accuracy: 0.7797\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.1528 - accuracy: 0.9738 - val_loss: 2.2292 - val_accuracy: 0.7792\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.1496 - accuracy: 0.9746 - val_loss: 2.2283 - val_accuracy: 0.7795\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1472 - accuracy: 0.9751 - val_loss: 2.2398 - val_accuracy: 0.7795\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.1451 - accuracy: 0.9755 - val_loss: 2.2437 - val_accuracy: 0.7794\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 151s 3s/step - loss: 0.1417 - accuracy: 0.9766 - val_loss: 2.2518 - val_accuracy: 0.7794\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.1399 - accuracy: 0.9767 - val_loss: 2.2615 - val_accuracy: 0.7792\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1376 - accuracy: 0.9770 - val_loss: 2.2684 - val_accuracy: 0.7798\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 145s 3s/step - loss: 0.1356 - accuracy: 0.9775 - val_loss: 2.2755 - val_accuracy: 0.7789\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.1326 - accuracy: 0.9780 - val_loss: 2.2641 - val_accuracy: 0.7792\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1307 - accuracy: 0.9786 - val_loss: 2.2796 - val_accuracy: 0.7795\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.1279 - accuracy: 0.9791 - val_loss: 2.2998 - val_accuracy: 0.7786\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1266 - accuracy: 0.9793 - val_loss: 2.3102 - val_accuracy: 0.7788\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.1246 - accuracy: 0.9796 - val_loss: 2.3186 - val_accuracy: 0.7787\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 154s 3s/step - loss: 0.1224 - accuracy: 0.9802 - val_loss: 2.3223 - val_accuracy: 0.7787\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.1205 - accuracy: 0.9804 - val_loss: 2.3248 - val_accuracy: 0.7787\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 154s 3s/step - loss: 0.1181 - accuracy: 0.9808 - val_loss: 2.3321 - val_accuracy: 0.7787\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.1162 - accuracy: 0.9810 - val_loss: 2.3312 - val_accuracy: 0.7788\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.1143 - accuracy: 0.9815 - val_loss: 2.3512 - val_accuracy: 0.7791\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.1122 - accuracy: 0.9821 - val_loss: 2.3557 - val_accuracy: 0.7783\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.1107 - accuracy: 0.9823 - val_loss: 2.3605 - val_accuracy: 0.7789\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.1100 - accuracy: 0.9821 - val_loss: 2.3629 - val_accuracy: 0.7785\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 152s 3s/step - loss: 0.1072 - accuracy: 0.9827 - val_loss: 2.3751 - val_accuracy: 0.7788\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 153s 3s/step - loss: 0.1053 - accuracy: 0.9831 - val_loss: 2.3722 - val_accuracy: 0.7787\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 154s 3s/step - loss: 0.1045 - accuracy: 0.9833 - val_loss: 2.3806 - val_accuracy: 0.7784\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.1019 - accuracy: 0.9837 - val_loss: 2.3955 - val_accuracy: 0.7790\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.1003 - accuracy: 0.9841 - val_loss: 2.4008 - val_accuracy: 0.7779\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.0996 - accuracy: 0.9839 - val_loss: 2.4059 - val_accuracy: 0.7788\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 149s 3s/step - loss: 0.0974 - accuracy: 0.9844 - val_loss: 2.4168 - val_accuracy: 0.7784\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 147s 3s/step - loss: 0.0956 - accuracy: 0.9848 - val_loss: 2.4150 - val_accuracy: 0.7782\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 148s 3s/step - loss: 0.0942 - accuracy: 0.9850 - val_loss: 2.4246 - val_accuracy: 0.7785\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.0930 - accuracy: 0.9852 - val_loss: 2.4417 - val_accuracy: 0.7790\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.0914 - accuracy: 0.9857 - val_loss: 2.4454 - val_accuracy: 0.7783\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 150s 3s/step - loss: 0.0894 - accuracy: 0.9859 - val_loss: 2.4461 - val_accuracy: 0.7781\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.0887 - accuracy: 0.9860 - val_loss: 2.4678 - val_accuracy: 0.7781\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.0887 - accuracy: 0.9857 - val_loss: 2.4518 - val_accuracy: 0.7785\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.0851 - accuracy: 0.9865 - val_loss: 2.4817 - val_accuracy: 0.7778\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 144s 3s/step - loss: 0.0844 - accuracy: 0.9866 - val_loss: 2.4793 - val_accuracy: 0.7778\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.0832 - accuracy: 0.9868 - val_loss: 2.4859 - val_accuracy: 0.7783\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 141s 3s/step - loss: 0.0826 - accuracy: 0.9868 - val_loss: 2.4867 - val_accuracy: 0.7781\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.0801 - accuracy: 0.9875 - val_loss: 2.4977 - val_accuracy: 0.7774\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.0792 - accuracy: 0.9875 - val_loss: 2.4963 - val_accuracy: 0.7778\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 0.0787 - accuracy: 0.9875 - val_loss: 2.5131 - val_accuracy: 0.7781\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.0773 - accuracy: 0.9878 - val_loss: 2.5220 - val_accuracy: 0.7780\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 138s 3s/step - loss: 0.0768 - accuracy: 0.9878 - val_loss: 2.5242 - val_accuracy: 0.7781\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 140s 3s/step - loss: 0.0751 - accuracy: 0.9879 - val_loss: 2.5192 - val_accuracy: 0.7784\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 138s 3s/step - loss: 0.0734 - accuracy: 0.9884 - val_loss: 2.5421 - val_accuracy: 0.7780\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 139s 3s/step - loss: 0.0732 - accuracy: 0.9884 - val_loss: 2.5351 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "trainSaveModel(model, model_path, encoder_input_data, decoder_input_data, decoder_target_data, batch_size,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444e495",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbddc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 将输入编码为状态向量。\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 生成长度为 1 的空目标序列。\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 用起始字符填充目标序列的第一个字符。\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 一批序列的采样循环\n",
    "    # (为了简化，这里我们假设一批大小为 1)。\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 采样一个 token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 退出条件：达到最大长度或找到停止符。\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目标序列（长度为 1）。\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新状态\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c3d239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向查询 token 索引可将序列解码回可读的内容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d252bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。\n",
      "-\n",
      "Input sentence: He calls the Green Book, his book of teachings, “the new gospel.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。\n",
      "-\n",
      "Input sentence: And the light breeze moves me to caress her long ear\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 微风推着我去爱抚它的长耳朵\n",
      "-\n",
      "Input sentence: They have the blood of martyrs is the White to flow …\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 它们的先烈们的鲜血是白流了…\n",
      "-\n",
      "Input sentence: Finally, the Lakers head to the Motor City to take on a Pistons team that currently owns the Eastern Conference's second best record (1/31). L.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。\n",
      "-\n",
      "Input sentence: \"The perfect match—my father loves names and Jackie loves money, \" sneered Alexander at the wedding. Neither he nor Christina ever got along with their stepmother17.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: “真是天造地设的一对——我父亲喜欢结交名人，杰姬酷爱金钱，”亚历山大在婚礼上讥讽道。他和克里斯蒂娜从未同他们的继母和睦相处过。\n",
      "-\n",
      "Input sentence: In 2006, Walmart was charged with racism when its recommendation engine paired Planet of the Apes with a documentary about Martin Luther King.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 2006年，沃尔玛的推荐引擎竟将《人猿星球》与马丁·路德·金的记录片配成了一对，为此沃尔玛遭到了种族歧视的指控。\n",
      "-\n",
      "Input sentence: The matte as main copper phase in the cleaning. slag was deter- mined by electron probe microscopic analysis.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 通过电子探针显微分析确定贫化渣中主要铜相为冰铜相。\n",
      "-\n",
      "Input sentence: Have you shined your shoes?\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 吉姆靠给人擦皮鞋为生。\n",
      "-\n",
      "Input sentence: The Tanning Matrix can be formed by resorcinol and oxazolidine E, and the reactioncharateristics between Tanning Matrix and collagen were investigated through NMR and size distribution analysis.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 用甘氨酸模拟胶原，研究间苯二酚-恶唑烷E鞣性基质的形成以及与胶原之间的反应特性。\n",
      "-\n",
      "Input sentence: Free delivery for addresses in the city. Can be delivered through Internet.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 免费市内取送、免费提供打印译稿及电子文档各一份。\n",
      "-\n",
      "Input sentence: Keele University is renowned for its exciting approach to higher education, beautiful campus, strong community spirit and excellent student life.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 基尔大学以其令人兴奋的方式高等教育，美丽的校园，强大的社区精神和优秀学生的生活。\n",
      "-\n",
      "Input sentence: Among them, there was the herb Tuckahoe grown in Yunnan and Guizhou provinces.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 这些中药中就有生长于云南和贵州的茯苓。\n",
      "-\n",
      "Input sentence: Your willingness to sacrifice countless late nights consoling them?\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 或者是因为你愿意花费无数的夜晚去安慰他们？\n",
      "-\n",
      "Input sentence: Callum: OK, we'll find out if you're right at the end of the programme.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: Callum：  OK，答案将会在节目的最后揭晓，到时我们再看你有没有答对。\n",
      "-\n",
      "Input sentence: When standing on a level surface, the hind feet are set back from under the body and the leg from pad to hock is at right angles to the ground.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 当他站在水平的地面上时，后足爪位于身躯后方（不在身躯正下方），从脚垫到飞节垂直于地面。\n",
      "-\n",
      "Input sentence: So who won? (Alaska doesn't count, you BOUGHT that state from Russia.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 所以到底是谁赢了？ 阿拉斯加不算数，那是你们从俄罗斯买的。\n",
      "-\n",
      "Input sentence: A Minneapolis couple decided to go to Florida to thaw out during a particularly icy winter.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimaldorviage说的情报》中下，这个没有与取代码的过程。                                                                   \n",
      "Correct sentence: 在一个特别寒冷的冬天，一对住在明尼阿波利斯市的夫妇决定去弗罗里达避寒。\n",
      "-\n",
      "Input sentence: Dumbledore, the lover of warm socks and sherbet lemons, creates soft, comfortable furniture.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 热爱温暖的毛袜子和冰冻柠檬汁的邓布利多，变出柔软舒适的家具。\n",
      "-\n",
      "Input sentence: To escape with a Ph.D., you must meaningfully extend the boundary of human knowledge.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 要成功拿到博士学位，你必须真正的扩展人类的知识边界。\n",
      "-\n",
      "Input sentence: Barry had been D.C.'s mayor for 12 years before he was put into prison for involvement with drugs in 1990.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimaldorviage说的情报》中下，这个没有与取代码的过程。                                                                   \n",
      "Correct sentence: 这人说：\"巴里在1990年由于涉及毒品而被关进监狱。 在这以前他在华盛顿担任了十二年的市长。\n",
      "-\n",
      "Input sentence: Phase out nankin/social security. It's not working and it's going to bankrupt the country.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 逐步淘汰社会保障。社会保障并不起作用，而且它会使我们的国家破产。\n",
      "-\n",
      "Input sentence: Daniel Radcliffe, who plays Harry, thanked all the fans who had turned up. \"If this doesn't get you exhilarated, nothing else will,\" he said.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 剧中哈里的饰演者丹尼尔·拉德克里夫对当天出席首映式的所有影迷表示了感谢，他说：“没有什么能比这更让你感到高兴的了。”\n",
      "-\n",
      "Input sentence: Look at these coasters over here.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 看看这边的杯垫。\n",
      "-\n",
      "Input sentence: It was tight. so ti lasted a long time .\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 两队不分胜负，所以持续了很长一段时间。\n",
      "-\n",
      "Input sentence: To use tone, press the YES button. You must use tone if you are setting up. the 2300 as a stand-alone stereo encoder.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 本帖隐藏的内容需要回复才可以浏览2300作为一个独立的立体声编码器。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Fuler is one of 253 schools have credited by the Sociation of Phiological schools in the United States and Canada.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 富勒是由美国和加拿大神学院联盟授权的253 家学院中的一员。\n",
      "-\n",
      "Input sentence: It shows that vertical stiffener's spaces have some effects on pure-shearing ulti…\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 而腹板鼓曲对纯弯和纯剪极限承载力的影响则可不予考虑。\n",
      "-\n",
      "Input sentence: \"People are embarrassed to admit that's why they're giving up their pets, \" said Betsy McFarland, the Humane Society's director of communications for companion animals.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: “人们羞于说出他们遗弃宠物的真正原因，”人道协会陪伴动物协调主管伊莉莎白. 麦克法兰说。\n",
      "-\n",
      "Input sentence: Mars gets hit in this tutorial complete with monolith from 2001 Space Odyssey all in 2D.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 火星在本教程中获得击中从2001年太空漫游所有的一块完整的二维。\n",
      "-\n",
      "Input sentence: Show all articles on this topic.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 显示所有关于这个主题的文章。\n",
      "-\n",
      "Input sentence: Yesterday, a city with the husband and wife suffering from AIDS in the city hospital to get two of their 18-month-old daughter of the AIDS antibody test results of the report alone.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 昨天，我市一对同患艾滋病的夫妻在市二院拿到了他们18个月大女儿的艾滋病抗体检测结果报告单。\n",
      "-\n",
      "Input sentence: A model wearing a traditional Korean hanbok performs in a water tank at the \"Underwater Hanbok Fashion Show\" in Seoul, South Korea.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 首尔“水下韩服时装展”上，一名穿着古装韩服的模特在水箱内表演着，韩国。\n",
      "-\n",
      "Input sentence: Haven't found some hurt you when it is not pain, you pay attention to it begins to faint do painful.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimaldorviage说的情报》中下，这个没有与取代码的过程。                                                                   \n",
      "Correct sentence: 有些伤你没发现的时候它不疼，等你一注意它就开始隐隐做痛。\n",
      "-\n",
      "Input sentence: First of all, the term \"justice\" can have different interpretations in language. That is, a language can define different connotations for \"justice.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 首先，「正义」可能在同一语言区中即已出现各种不同的解释，亦即使用同样语言者都可能对「正义」有不同的内涵认定；\n",
      "-\n",
      "Input sentence: Standard Edition: for small-scale applications that require data caching and sharing clustered data.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 标准版：针对需要数据缓存和共享集群数据的小规模应用。\n",
      "-\n",
      "Input sentence: So basically our L/Cs are no different then other sight L/Cs.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 客户的意思是远期信用证吗？但是有不符合远期信用证的规定（银行利息由买方负责）\n",
      "-\n",
      "Input sentence: Through parameter calibration and model validation, model can be adapted to morphogenesis and LAI simulation for different varieties and management practice.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 通过对模型参数的校正和核实，使模型适应于不同类型品种的形态发生和LAI动态模拟。\n",
      "-\n",
      "Input sentence: NH3-N and concentration of particles reached lnd standard of sewage treatment plant.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 浓度和浊度达到国家一级排放标准。\n",
      "-\n",
      "Input sentence: Due to the high tonnage and large span of the whole steel space frame, process and fabrication of the steel space frame shall be strictly precise.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 由于整个钢网架的吨位重、跨度大，所以，对钢网架的加工制作精度方面要求非常严格。\n",
      "-\n",
      "Input sentence: As another example, the Japanese traditional \"soup\" (ie take a bath Church) is the mixing of men and women bath in some places so far, and often not prepared to foreigners, \"red in the face.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 再比如，日本传统的“汤”（即洗澡堂）是男女混浴的，有些地方至今如此，常让没有思想准备的外国人“面红耳赤”。\n",
      "-\n",
      "Input sentence: The theme ofWWIIwill always remain actual as the war will always be remembered by off-springs of those who won it.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 二战主题将会经久不衰，因为战胜一方的子孙后代将永远铭记这场战争。\n",
      "-\n",
      "Input sentence: No suspected cancer cells or cancer cells were found.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 宫颈刮片检查两组均未查见可疑癌细胞或癌细胞。\n",
      "-\n",
      "Input sentence: As our quotation is based on sea extra charge for dispatch by parcel post should be borne by buyers.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimaldorviage说的情报》中下，这个没有与取代码的过程。                                                                   \n",
      "Correct sentence: 咱们所报价钱是以海运为根基的。因此，因货品以邮包形式寄发而发生的所有分外花销应由买户承担。\n",
      "-\n",
      "Input sentence: Three bright spots , namely: China? Anping International Wire Mesh Fair, Anping international wire mesh Anping wire mesh industry base and the World.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 三大亮点，即：中国?安平国际丝网博览会、安平国际丝网产业基地和安平丝网大世界。\n",
      "-\n",
      "Input sentence: The ever-intensifying contradictions between economic development and resources and environment must be solved earnestly.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 经济发展与资源、环境的矛盾日益尖锐的情况亟待改变。\n",
      "-\n",
      "Input sentence: Still, who are we to say that we can stay?\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 然而，谁有敢说我们能长留此地？\n",
      "-\n",
      "Input sentence: Then, in analogy with the annealing of metals, the temperature is made high in the early stages of the process for faster minimisation or learning, then is reduced for greater stability.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 然后，与金属退火原理相类似，在开始阶段为了更快地最小化或学习，温度被升得很高，然后才（慢慢）降温以求稳定。\n",
      "-\n",
      "Input sentence: In these few years, the vehicle are rapidly increasing, the different parking are becoming more and more outstanding. The automatic parking system would provide a good way to set down the problems.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 近年来，随着机动车数量的急剧增长，国内许多城市停车难的问题越来越突出，而自动化立体车库的出现将为解决城市停车难的问题提供一个很好的方案。\n",
      "-\n",
      "Input sentence: Well my name is lee i am 30yrs my interest is movie, basketball, roller skating, shooting pool, bowling, music, & going on long walks in the park with my dog diamond.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 我的名字是李，今年30岁了，爱好是看电影，打篮球，溜旱冰，跳水，打保龄，听歌以及同我的狗宝石一起在公园散步。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: He had formerly been in business at Bristol, but failed in debt to a number of people, compounded, and went to America.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 在那里他专心一志地做生意，在几年中就赚到许多钱。\n",
      "-\n",
      "Input sentence: Basing on the on site tests of anchor, authors found that anchors have obvious pre stress loss problem during stretching and locking, analyzed and proposed several solutions.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 根据对锚杆的现场测试，发现锚杆在张拉及锁定时存在显著的预应力损失问题，并对此进行了分析，提出了解决问题的几个办法。\n",
      "-\n",
      "Input sentence: From hair tip first began gradually, after all, through from downward, nodular comb.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimalord antiterare说。说没说“无法与行最佳的策略uri。                                                              \n",
      "Correct sentence: 先从发梢开始通顺，逐渐向上，打通全部结节后，从上向下梳顺。\n",
      "-\n",
      "Input sentence: The sky began to be clear up a bit when we left St Gallen abbey and library.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 离开圣加仑修道院和修道院图书馆，天稍稍开始放晴。\n",
      "-\n",
      "Input sentence: Once more, Cinderella's fairy godmother reminded her to be home by midnight.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 仙女教母再一次提醒辛德瑞拉要在午夜前到家。\n",
      "-\n",
      "Input sentence: This paper introduces the demand analysis and function design in detail, gives the source codes of relevant interface functions and base algorithm.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 本文详细介绍了它的需求分析和功能分析，并给出了相关接口函数和底层算法函数的实现源代码。\n",
      "-\n",
      "Input sentence: Manufacturer of thin and ultra-thin non-ferrous metal foils mainly made of copper, copper-alloys, nickel, genuine silver and nickel-silver.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 舒伦克超薄铜合金是制造超薄非铁合金箔为主的制造商。\n",
      "-\n",
      "Input sentence: All day thy wings have fann'd At that far height, the cold thin atmosphere: Yet stoop not, weary, to the welcome land, Though the dark night is near.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 你成天翕动翅膀， 任空气稀薄暴风寒冷，飞在高处， 疲乏中你不肯降落舒适的大地， 即使黑夜即将紧闭它的帷幕。\n",
      "-\n",
      "Input sentence: The two other attackers are believed to have tried to enter the terminal, which is protected by heavily armed police and X-ray machines.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 另外两名自杀手据信试图进入有武装警察和X光检测机保护的航站楼。\n",
      "-\n",
      "Input sentence: He became in legitimately through the door of the Law (vv. 1-3).\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 祂是合法地从律法的门进来的（1～3）；\n",
      "-\n",
      "Input sentence: The effect mechanism of laser biology was systemotically and deeply discussed in this artical, from 4 aspects:the light, electromagnetism field, heat and pressure effect of laser.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 本文对激光的生物学作用机制，从激光的光、电磁、热和压力效应四个方面进行了讨论。\n",
      "-\n",
      "Input sentence: The variant of FOXO3A associated with longevity is much more prevalent in 100-year-olds even than in 95-year-olds, which clearly demonstrates the value of studying the centenarian genome.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 与寿命相关的FOXO3A在百岁老人中即使与95岁老人相比也要普遍得多，这也清楚地显示出研究百岁老人基因组的价值所在。\n",
      "-\n",
      "Input sentence: Again, Uruguay are slight exception – they did start with a three-man defence.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 乌拉圭例外—他们一开始是个三后卫阵型。\n",
      "-\n",
      "Input sentence: Rugby' Seven People System origined from Scotland, it has special regularity and character.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 七人制橄榄球属于英式橄榄球，它具有独特的规律和特点。\n",
      "-\n",
      "Input sentence: This month you will be the darling of the media, so try to be a guest on TV and radio, or try for an interview or write-up on the Inte or in print.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 本月可能与媒体打道，所以测验考试作为电视或电台的嘉宾，或者为网络或最简单的面媒体采访和撰写。\n",
      "-\n",
      "Input sentence: So Isay to you that a novel must stand up to the adult tests of reality.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 所以我对你说，小说必须要成熟起来，能够让成年人将之放在生活中试验。\n",
      "-\n",
      "Input sentence: Yeah, like gentle breeze blowing through the cheeks, the hair dancing in the wind.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimalord ant说。                                                                                      \n",
      "Correct sentence: 是的，像微风吹过面颊，因为发梢在跳舞。\n",
      "-\n",
      "Input sentence: At present the direction of travel is not fully clear, but Theresa May's government has promised to set out a plan before triggering the EU's Article 50 divorce procedure.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 目前轨迹尚不清晰，但特雷莎·梅的政府承诺将在启动欧盟第50条脱欧程序之前制订一项计划。\n",
      "-\n",
      "Input sentence: Both the theoretical and experimental results have shown that there is a constant water level difference between the refined dynamic water level and the static one in the same well.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 理论分析和实验表明，改性动水位与同一井孔对应的静水位之间相差一恒定水位落差。\n",
      "-\n",
      "Input sentence: Otwoma believes the expense of generating nuclear energy could one day be met through shared regional projects but, until then, Kenya has to move forward on its own.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: Otwoma认为，核能发电的成本可能有朝一日通过共享的区域项目分担，但是在那之前肯尼亚需要自主前进。\n",
      "-\n",
      "Input sentence: On July 14, the Kremlin announced it will suspend participation in the Treaty on Conventional Forces in Europe, or CFE for short.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 7月14日，克里姆林宫宣布它将暂停参《与欧洲常规武器条约（CFE）》。\n",
      "-\n",
      "Input sentence: Outside-left. Made United debut at 17.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 左外锋，17岁就为曼联初次登场。\n",
      "-\n",
      "Input sentence: When the soul descends it divides itself creating a male and female half-soul.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 当灵魂下降并分离自己，创造了一个男性和女性的半个灵魂。\n",
      "-\n",
      "Input sentence: Axis symmetrical pure radial and pure shear vibrations were investigated theoretically for disk concentrators, whose thickness varying step-wise, linearly or exponentially with radial distance.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 理论上研究了盘形聚能器的轴对称纯径向振动和纯扭转振动。 所用盘形聚能器的厚度，沿半径按阶梯形，锥形和指数形变化。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: The idea of flipping from one entry to another, following a line of inquiry (especially etymological inquiry) from one page to another, even one volume to another, is a sensual experience.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 想象一下从一个词条翻到另外一个词条，顺着线索（尤其是词源的查询）从一页翻到另一页，从这一卷翻到另外一卷，（绝对）是一种感官体验。\n",
      "-\n",
      "Input sentence: Further Practice for Pairs ·Add a third speaker and create your own lines.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 加进圈外人，创作你们自己的对话。\n",
      "-\n",
      "Input sentence: Still, Brasier asserts that the light carbon enrichments may well be able to form through lifeless chemical reactions—much as Fedo and others have argued could have occurred at Akilia.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 但伯拉西尔仍旧坚称，无生命的化学反应应该也可以聚集很多轻碳，这跟费多等人对阿基利亚的论辩很类似。\n",
      "-\n",
      "Input sentence: The company is through ISO9001 quality system authentication , some products have also passed UL, CCEE authentication .\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是过节中国一节来已经发明。                                                                                                     \n",
      "Correct sentence: 公司通过了ISO9001质量体系认证，部份产品还通过了UL、CCEE认证。\n",
      "-\n",
      "Input sentence: The Book of Revelation was also traditionally assigned to him.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 〈启示录〉传统上也认为是他的作品。\n",
      "-\n",
      "Input sentence: Our guest bedroom has an entire wall stacked with boxes containing unknown objects of more “stuff”.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 在我们的来宾卧室里，各种装着未知东西的箱子堆满了整堵墙。\n",
      "-\n",
      "Input sentence: It was easily good enough for pole so that was the main thing.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 那一圈跑得不错，足以使我获得杆位，那是最主要的。\n",
      "-\n",
      "Input sentence: This instrument uses the hardware structure taking 8031 chip-microprocessor as a main. It has functions of self-diagnosis, digital filtering and non-linear compensation etc.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 该仪器采用以8031单片机为核心的硬件结构，它具有自诊断、数字滤波和非线性补偿等功能。\n",
      "-\n",
      "Input sentence: While these space rocks don't exactly share our planet's orbit, they do cross it, in the sense that when they are closest to the sun, they are inside Earth's orbital path.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 虽然这些小行星并未和地球共享轨道，他们却实际上穿过了地球轨道，当他们靠近近日点的时候，他们则在地球绕日轨道内侧。\n",
      "-\n",
      "Input sentence: John has a windfall . It surprises his wife greatly .\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 约翰意外获得一笔财富，使他夫人大吃一惊。\n",
      "-\n",
      "Input sentence: So, the article researched the geography distribution of poets from 712AD to 805AD according to the fifteen Dao and analyzed the poets that existed in the same time and room.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 用唐代实行的行政规划十五道为基础，对公元712——805年诗人的地理分布进行全国性的分道研究，在时间、空间一定的前提下，对唐诗人进行定量、定性分析。\n",
      "-\n",
      "Input sentence: Yet he's ready to move on, knowing that \"the causes I care about have campaign-tested technology to work with.\"\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 然而他已经准备好了前行，他明白，“他所在意的那些已经经过了竞选活动的测试。”\n",
      "-\n",
      "Input sentence: Kandahar provincial official and his bodyguard on their way to work were shot dead by two gunmen on a motorbike.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 坎大哈省官员阿吉斯塔尼和他的保镖在上班的路上被两名骑摩托车的武装分子开枪打死。\n",
      "-\n",
      "Input sentence: We've rounded up some unusual ways to put your bottom-shelf vodka to good use all around your house.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--负-20节会-1.-tETmartimaldorviage说的情报》中下，这个没有与取代码的过程。                                                                   \n",
      "Correct sentence: 我们搜集了一些可以让你把箱底的伏特加应用到家里各个角落的小方法。\n",
      "-\n",
      "Input sentence: The target of anti_monopoly law should be to contain monopolizing behavior and various restrictive practices.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 反垄断法应以垄断和其他限制竞争行为作为规制对象；\n",
      "-\n",
      "Input sentence: To build a conservation-minded society, we should act at present for our future.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 构建节约型社会，是以现代的目光，着眼于长远的未来，是当务之急。\n",
      "-\n",
      "Input sentence: Large doses of carbs, sugar, and caffeine might keep you awake for a short time, but they will eventually lead to a \"crash, \" and have the opposite effect.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 大剂量的碳水化合物、糖和咖啡因会让你清醒一段时间，但是他们最终将导致一个觧“重磅睡眠”，产生相反的效果。\n",
      "-\n",
      "Input sentence: We desperately need a nation to exert some leadership, adopting policies to move promptly in that direction.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在一种 合头boremten--命因是对无节目的信息。                                                                                                         \n",
      "Correct sentence: 我们急需一个国家发挥一定的领导作用，沿这一方向迅速采取措施。\n",
      "-\n",
      "Input sentence: Environmental records shall be stored and maintained in such a way that they are readily retrievable and protected against damage, deterioration or loss.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 对环境记录的保存和管理应使之便于查阅，避免损坏、变质或遗失。\n",
      "-\n",
      "Input sentence: He hastily composed another post, and then spent twenty minutes rephrasing it in a calmer tone, but a day later, when that message had also been deleted, his rage erupted.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 范老师急忙构思了又一条帖子，然后花了二十分钟时间把措辞改写得比较平和。 但是一天之后，这条留言又被删除了，这时范老师的愤怒爆发了。\n",
      "-\n",
      "Input sentence: Results(1)Determining the morbidity of hyperbilirubinemia; It put up an extremely remarkable difference comparing the antibody-released test result being positive group to the control group(P<0.01).\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 结果（1）抗体释放试验阳性患儿与对照组新生儿高胆红素血症发生率差异有统计学意义（P<0.01）。\n",
      "-\n",
      "Input sentence: All eastbound trains have been cancelled due to faulty signals.\n",
      "Decoded sentence: 大鼠的文章节目才分别 在 1 2 中                                                                                                                             \n",
      "Correct sentence: 所有向东开去的火车由于信号错误而均被取消。\n",
      "-\n",
      "Input sentence: Taken into account the fact that aggregates absorb pitch, required abilities to resist high temperature track and penetration could be gained by controlling interspace ratio (4%).\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 考虑集料对沥青的吸收，用混合料的设计空隙率（4％）控制混合料的高温抗车辙能力与渗水性。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Unlike many of the other pirate-radio operators, who were in it mostly for money or adventure, Smedley saw his broadcasts as part of a wider moral crusade.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.24；                                                                                                                           \n",
      "Correct sentence: 许多海盗电台经营者常常都是为了金钱或冒险而入行，斯梅德利却与之不同，他将其广播事业视为广泛道德运动的一部分。\n",
      "-\n",
      "Input sentence: Ran Hua (1961 ~), female, associate professor, PhD. candidate , School of Journalism & Communication, Wuhan University, majoring in communication theories.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 冉华（1961～），女，武汉大学新闻与传播学院副教授，在职博士生，主要从事传播理论研究。\n",
      "-\n",
      "Input sentence: Others include shrouding Earth in sun-reflecting aerosol particles, manufacturing CO2-absorbing artificial trees, and pumping CO2 into underground reservoirs.\n",
      "Decoded sentence: 大鼠的文章节目才分别用 2 19.2%；                                                                                                                           \n",
      "Correct sentence: 还有其它的计策，比如用具有反光效应的气溶胶粒子覆盖地球，或生产吸收二氧化碳的人工树木，还有把二氧化碳倒入地下水库的计策。\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # 抽取一个序列（训练集的一部分）进行解码。\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Correct sentence:', target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98cf1ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a0ea603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2c26818e2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a31359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f2cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f5d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae383aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45e051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6a4c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f7a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
